{"cells":[{"cell_type":"markdown","metadata":{},"source":["  # Introduction\n","\n","  This is a copy of the  example given on the Deeptime documentation website [here](https://deeptime-ml.github.io/latest/notebooks/examples/ala2-example.html).  Run this example first before trying any of the experimental code to make sure everting is working as it should!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from copy import deepcopy\n","import os\n","from click import progressbar\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import mdshare  \n","\n","from deeptime.util.types import to_dataset\n","from tqdm.notebook import tqdm  # progress bar\n","from deeptime.decomposition.deep import VAMPNet\n","from torch.utils.data import DataLoader\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[" # Hyperparameters\n","\n","  30% validation split\n","  10000 batch size\n","  lagtime of 1\n","  Network architecture:\n","  Input layer is 60, corresponding to the number of atomic coordinates of the heavy atoms of alanine tripeptide\n","  There are 20 heavy atoms in alanine tripeptide, and each has an X, Y, and Z coordinate, giving 60 data points total.\n","  There follows an intermediate layer 60x60, then another 60x100\n","  There are then four hidden layers shaped 100x100\n","  Then a layer 100x60\n","  And finally a layer 60x6, giving 6 output states.\n","  Alanine tripeptide's ramachandran plot shows 6 distinct metastable states.\n","  See validation loss below the training/validation graph\n","\n","\n","  try changing learning rate\n","  add horozontal line to validation chart to show max score (n_states)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["assert torch.cuda.is_available(), \"Need a GPU with CUDA\" \n","device = torch.device(\"cuda\")\n","torch.backends.cudnn.benchmark = True\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["  ## Globals"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_dir = \"/home/matthewb/online_vampnets/data\"\n","data_dir = \"/home/matthewb/ANI-Peptides/long_sims_final/solvated_amber/trajectory_split_113452_120422\"\n","lag_time = 1\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["file_coords = os.path.join(data_dir, \"coords_archive.npz\")\n","with np.load(file_coords) as fh:\n","    data_coords = fh[\"arr_0\"].astype(np.float32)\n","\n","file_dihedral = os.path.join(data_dir, \"dihedrals_archive.npz\")\n","with np.load(file_dihedral) as fh:\n","    data_dihedral = fh[\"arr_0\"].astype(np.float32)\n","\n","print(data_coords.shape)\n","print(data_dihedral.shape)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["f, ax = plt.subplots(1, 1)\n","\n","x = data_dihedral[:, 0, :].flat\n","y = data_dihedral[:, 1, :].flat\n","hb = ax.hexbin(x, y, mincnt=5)\n","ax.set_aspect('equal')\n","cb = f.colorbar(hb, ax=ax)\n","cb.set_label('# of frames in bin')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["  The following function splits up the trajectory `data_coords` into pairs (X_t, X_t+lagtime) and then puts all pairs from the trajectory into a single generator.\n","\n","  So for lagtime = 1:\n","   - The trajectory data is shape (x, 60), x is the length of the traj (sampled every ps) and 60 being the number of coordinates\n","   - The dataset will have length x-1, each item will be a tuple of length 2 (Xt, Xt+1), and each tuple item will have length 60\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(data_coords.shape)\n","dataset = to_dataset(data=data_coords, lagtime=1)\n","print(len(dataset), len(dataset[0]), len(dataset[0][0]))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_coords = data_coords.shape[1]\n","n_val = int(len(dataset)*.3)\n","train_data, val_data = torch.utils.data.random_split(dataset, [len(dataset) - n_val, n_val])\n","n_output_states = 6\n","n_epochs = 4\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lobe = nn.Sequential(\n","    nn.BatchNorm1d(data_coords.shape[1]),\n","    nn.Linear(data_coords.shape[1], n_coords), nn.ELU(),\n","    nn.Linear(n_coords, 100), nn.ELU(),\n","    nn.Linear(100, 100), nn.ELU(),\n","    nn.Linear(100, 100), nn.ELU(),\n","    nn.Linear(100, 100), nn.ELU(),\n","    nn.Linear(100, 100), nn.ELU(),\n","    nn.Linear(100, n_coords), nn.ELU(),\n","    nn.Linear(n_coords, n_output_states),\n","    nn.Softmax(dim=1)  # obtain fuzzy probability distribution over output states\n",")\n","\n","lobe_timelagged = deepcopy(lobe).to(device=device)\n","lobe = lobe.to(device=device)\n","print(lobe)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vampnet = VAMPNet(lobe=lobe, lobe_timelagged=lobe_timelagged, learning_rate=5e-3, device=device)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loader_train = DataLoader(train_data, batch_size=1000, shuffle=True)\n","loader_val = DataLoader(val_data, batch_size=len(val_data), shuffle=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model = vampnet.fit(loader_train, n_epochs,\n","#                     validation_loader=loader_val, progress=tqdm).fetch_model()\n","model = vampnet.fit(loader_train, n_epochs,\n","                    validation_loader=loader_val).fetch_model()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.loglog(*vampnet.train_scores.T, label='training')\n","plt.loglog(*vampnet.validation_scores.T, label='validation')\n","plt.xlabel('step')\n","plt.ylabel('score')\n","plt.legend()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_score = max(vampnet.train_scores[:, 1])\n","print(f\"Number of epochs: {n_epochs}\")\n","print(f\"Number of output states: {n_output_states}\")\n","print(f\"Final validation score: {validation_score}\")\n","print(f\"Validation loss: {100*(n_output_states - validation_score):.3f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["state_probabilities = model.transform(data_coords)\n","print(state_probabilities.shape)\n","print(data_coords.shape)\n","print(data_dihedral.shape)\n","x = data_dihedral.reshape(len(data_dihedral), -1).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["state_probabilities = model.transform(data_coords)\n","# state_probabilities = np.vstack([state_probabilities])\n","print(state_probabilities.shape)\n","print(data_dihedral.shape)\n","f, axes = plt.subplots(n_output_states, 3, figsize=(18, (18//3*n_output_states)))\n","for i, ax in enumerate(axes.flatten()):\n","    ax.scatter(\n","        data_dihedral[:, 0, i%3][::50], \n","        data_dihedral[:, 1, i%3][::50], \n","        c=state_probabilities[:, i//3][::50],\n","        cmap=\"plasma\",\n","        s=.2,\n","        alpha=.5\n","    )\n","    ax.set_title(f'State {i//3}, Dihedral {i%3}')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["state_probabilities = model.transform(data_coords)\n","for ix, (mini, maxi) in enumerate(zip(np.min(state_probabilities, axis=0),\n","                                      np.max(state_probabilities, axis=0))):\n","    print(f\"State {ix+1}: [{mini}, {maxi}]\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["assignments = state_probabilities.argmax(1)\n","f, axes = plt.subplots(3, 1, figsize=(6, 18))\n","for i, ax in enumerate(axes.flatten()):\n","    ax.scatter(*data_dihedral[:, :, i][::50].T, c=assignments[::50], s=5, alpha=.2)\n","    ax.set_title(f'Transformed state assignments: Dihedral {i}')\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from deeptime.decomposition import VAMP\n","\n","vamp_estimator = VAMP(lagtime=1, observable_transform=model)\n","vamp = vamp_estimator.fit(data_coords).fetch_model()\n","print(f\"VAMP-2 score under lag 10: s = {vamp.score(2).round(2)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lagtimes = np.arange(1, 201, 10, dtype=np.int32)\n","timescales = []\n","for lag in lagtimes:\n","    ts = VAMP(lagtime=lag, observable_transform=model).fit(data_coords).fetch_model().timescales(k=5)\n","    timescales.append(ts)\n","\n","f, ax = plt.subplots(1, 1)\n","ax.semilogy(lagtimes, timescales)\n","ax.set_xlabel('lagtime')\n","ax.set_ylabel('timescale / step')\n","ax.fill_between(lagtimes, ax.get_ylim()[0]*np.ones(len(lagtimes)), lagtimes, alpha=0.5, color='grey')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vamp_estimator = VAMP(lagtime=1, observable_transform=model)\n","vamp = vamp_estimator.fit(data_coords).fetch_model()\n","\n","validator = vamp_estimator.chapman_kolmogorov_validator(mlags=20)\n","cktest = validator.fit(data_coords, n_jobs=1).fetch_model()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_states = len(vamp.singular_values)\n","\n","tau = cktest.lagtimes[1]\n","steps = len(cktest.lagtimes)\n","fig, ax = plt.subplots(n_states, n_states, sharex=True, sharey=True, constrained_layout=True)\n","for i in range(n_states):\n","    for j in range(n_states):\n","        pred = ax[i][j].plot(cktest.lagtimes, cktest.predictions[:, i, j], color='b')\n","        est = ax[i][j].plot(cktest.lagtimes, cktest.estimates[:, i, j], color='r', linestyle='--')\n","        ax[i][j].set_title(str(i+1)+ '->' +str(j+1),\n","                                       fontsize='small')\n","ax[0][0].set_ylim((-0.1,1.1))\n","ax[0][0].set_xlim((0, steps*tau))\n","ax[0][0].axes.get_xaxis().set_ticks(np.round(np.linspace(0, steps*tau, 3)));\n","fig.legend([pred[0], est[0]], [\"Predictions\", \"Estimates\"], 'lower center', ncol=2,\n","           bbox_to_anchor=(0.5, -0.1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["OVERLAID_FRAMES = 100\n","# select n frames from each state which are most strongly predicted to be this state\n","\n","states = []\n","for i in range(n_output_states):\n","    state_i = state_probabilities[:, i].argsort()[-OVERLAID_FRAMES:]\n","    print(f\"state {i}:\", state_i[:10], \"...\", state_i.shape)\n","    states.append(state_i)\n","\n","import mdtraj\n","traj_path = os.path.join(os.path.dirname(data_dir), \"trajectory.dcd\")\n","top_path = os.path.join(os.path.dirname(data_dir), \"topology.pdb\")\n","assert os.path.isfile(traj_path)\n","assert os.path.isfile(top_path)\n","\n","traj = mdtraj.load(traj_path, top=top_path)\n","for state_idx, state in enumerate(states):\n","    state_traj_savename = os.path.join(data_dir, f\"state_{state_idx}.dcd\")\n","    # reference = None\n","    with mdtraj.open(state_traj_savename, mode=\"w\") as fh:\n","        for frame_counter, frame_idx in enumerate(state):\n","            print(f\"state {state_idx} of {len(states)}, frame {frame_counter} of {OVERLAID_FRAMES}\", end=\"     \\r\")\n","            # frame = mdtraj.load_frame(traj_path, frame_idx, top_path)\n","            frame = traj[frame_idx]\n","            # if not reference:\n","            #     reference = frame\n","            # frame = frame.superpose(reference)\n","            fh.write(\n","                frame.xyz, \n","                cell_lengths = frame.unitcell_lengths, \n","                cell_angles = frame.unitcell_angles\n","            )\n"]}],"metadata":{"interpreter":{"hash":"b782830d14801990968d50b67801723d22e93a81894226cf91e015176810a417"},"kernelspec":{"display_name":"Python 3.9.10 ('ani')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
